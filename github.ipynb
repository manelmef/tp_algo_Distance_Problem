{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1607bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des documents O(max(n1,n2))\n",
    "with open('C:/Users/khadidja/Documents/jupyter/doc1.txt', 'r', encoding='utf-8') as d1:\n",
    "    doc1 = d1.read()\n",
    "\n",
    "with open('C:/Users/khadidja/Documents/jupyter/doc2.txt', 'r', encoding='utf-8') as d2:\n",
    "    doc2 = d2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcc0eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La', 'POO', 'ou', 'programmation', 'orient√©e', 'objet', 'est', 'un', 'concept', 'fondamental', 'en', 'informatique', 'Elle', 'repose', 'sur', \"l'encapsulation\", 'des', 'donn√©es', 'et', 'des', 'm√©thodes', 'dans', 'des', 'objets', 'ce', 'qui', 'favorise', 'la', 'modularit√©', 'du', 'code', 'et', 'la', 'gestion', 'de', 'projets', 'logiciels', 'complexes']\n",
      "['La', 'programmation', 'orient√©e', 'objet', 'POO', 'est', 'un', 'paradigme', 'de', 'programmation', 'largement', 'utilis√©', 'dans', 'le', 'd√©veloppement', 'de', 'logiciels', 'Elle', 'repose', 'sur', 'la', 'cr√©ation', 'de', 'classes', 'et', \"d'objets\", 'permettant', 'une', 'meilleure', 'organisation', 'du', 'code', 'et', 'la', 'r√©utilisation', 'de', 'modules']\n"
     ]
    }
   ],
   "source": [
    "# Tokeniser le texte en mots, en le divisant par des espaces et en supprimant la ponctuation O(n + m),n longueur document en caract√®res m longueur des mots\n",
    "tokendoc1 = doc1.split()\n",
    "tokendoc1 = [word.strip('.,!?()[]{}\"\\'') for word in tokendoc1]\n",
    "tokendoc2 = doc2.split()\n",
    "tokendoc2 = [word.strip('.,!?()[]{}\"\\'') for word in tokendoc2]\n",
    "print(tokendoc1)\n",
    "print(tokendoc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3a4f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict1 = {'La': 1, 'POO': 1, 'ou': 1, 'programmation': 1, 'orient√©e': 1, 'objet': 1, 'est': 1, 'un': 1, 'concept': 1, 'fondamental': 1, 'en': 1, 'informatique': 1, 'Elle': 1, 'repose': 1, 'sur': 1, \"l'encapsulation\": 1, 'des': 3, 'donn√©es': 1, 'et': 2, 'm√©thodes': 1, 'dans': 1, 'objets': 1, 'ce': 1, 'qui': 1, 'favorise': 1, 'la': 2, 'modularit√©': 1, 'du': 1, 'code': 1, 'gestion': 1, 'de': 1, 'projets': 1, 'logiciels': 1, 'complexes': 1}\n",
      "dict2 = {'La': 1, 'programmation': 2, 'orient√©e': 1, 'objet': 1, 'POO': 1, 'est': 1, 'un': 1, 'paradigme': 1, 'de': 4, 'largement': 1, 'utilis√©': 1, 'dans': 1, 'le': 1, 'd√©veloppement': 1, 'logiciels': 1, 'Elle': 1, 'repose': 1, 'sur': 1, 'la': 2, 'cr√©ation': 1, 'classes': 1, 'et': 2, \"d'objets\": 1, 'permettant': 1, 'une': 1, 'meilleure': 1, 'organisation': 1, 'du': 1, 'code': 1, 'r√©utilisation': 1, 'modules': 1}\n"
     ]
    }
   ],
   "source": [
    "# Mettre a jour les frequencies O(len(tokendoc1) + len(tokendoc2))\n",
    "dict1 = {}\n",
    "dict2 = {}\n",
    "\n",
    "for word in tokendoc1:\n",
    "    if word not in dict1:\n",
    "        dict1[word] = 1\n",
    "    else:\n",
    "        dict1[word] += 1\n",
    "for word in tokendoc2:\n",
    "    if word not in dict2:\n",
    "        dict2[word] = 1\n",
    "    else:\n",
    "        dict2[word] += 1\n",
    "print(\"dict1 =\",dict1)\n",
    "print(\"dict2 =\",dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90d7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict1 = {'La': 1, 'POO': 1, 'ou': 1, 'programmation': 1, 'orient√©e': 1, 'objet': 1, 'est': 1, 'un': 1, 'concept': 1, 'fondamental': 1, 'en': 1, 'informatique': 1, 'Elle': 1, 'repose': 1, 'sur': 1, \"l'encapsulation\": 1, 'des': 3, 'donn√©es': 1, 'et': 2, 'm√©thodes': 1, 'dans': 1, 'objets': 1, 'ce': 1, 'qui': 1, 'favorise': 1, 'la': 2, 'modularit√©': 1, 'du': 1, 'code': 1, 'gestion': 1, 'de': 1, 'projets': 1, 'logiciels': 1, 'complexes': 1, 'paradigme': 0, 'largement': 0, 'utilis√©': 0, 'le': 0, 'd√©veloppement': 0, 'cr√©ation': 0, 'classes': 0, \"d'objets\": 0, 'permettant': 0, 'une': 0, 'meilleure': 0, 'organisation': 0, 'r√©utilisation': 0, 'modules': 0}\n",
      "dict2 = {'La': 1, 'programmation': 2, 'orient√©e': 1, 'objet': 1, 'POO': 1, 'est': 1, 'un': 1, 'paradigme': 1, 'de': 4, 'largement': 1, 'utilis√©': 1, 'dans': 1, 'le': 1, 'd√©veloppement': 1, 'logiciels': 1, 'Elle': 1, 'repose': 1, 'sur': 1, 'la': 2, 'cr√©ation': 1, 'classes': 1, 'et': 2, \"d'objets\": 1, 'permettant': 1, 'une': 1, 'meilleure': 1, 'organisation': 1, 'du': 1, 'code': 1, 'r√©utilisation': 1, 'modules': 1, 'ou': 0, 'concept': 0, 'fondamental': 0, 'en': 0, 'informatique': 0, \"l'encapsulation\": 0, 'des': 0, 'donn√©es': 0, 'm√©thodes': 0, 'objets': 0, 'ce': 0, 'qui': 0, 'favorise': 0, 'modularit√©': 0, 'gestion': 0, 'projets': 0, 'complexes': 0}\n"
     ]
    }
   ],
   "source": [
    "for mot in dict1:\n",
    "    if mot not in dict2:\n",
    "        dict2[mot]=0\n",
    "        \n",
    "for mot in dict2:\n",
    "    if mot not in dict1:\n",
    "        dict1[mot]=0   \n",
    "       \n",
    "print(\"dict1 =\",dict1)\n",
    "print(\"dict2 =\",dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e44c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict1 = {'Elle': 1, 'La': 1, 'POO': 1, 'ce': 1, 'classes': 0, 'code': 1, 'complexes': 1, 'concept': 1, 'cr√©ation': 0, \"d'objets\": 0, 'dans': 1, 'de': 1, 'des': 3, 'donn√©es': 1, 'du': 1, 'd√©veloppement': 0, 'en': 1, 'est': 1, 'et': 2, 'favorise': 1, 'fondamental': 1, 'gestion': 1, 'informatique': 1, \"l'encapsulation\": 1, 'la': 2, 'largement': 0, 'le': 0, 'logiciels': 1, 'meilleure': 0, 'modularit√©': 1, 'modules': 0, 'm√©thodes': 1, 'objet': 1, 'objets': 1, 'organisation': 0, 'orient√©e': 1, 'ou': 1, 'paradigme': 0, 'permettant': 0, 'programmation': 1, 'projets': 1, 'qui': 1, 'repose': 1, 'r√©utilisation': 0, 'sur': 1, 'un': 1, 'une': 0, 'utilis√©': 0}\n",
      "dict2 = {'Elle': 1, 'La': 1, 'POO': 1, 'ce': 0, 'classes': 1, 'code': 1, 'complexes': 0, 'concept': 0, 'cr√©ation': 1, \"d'objets\": 1, 'dans': 1, 'de': 4, 'des': 0, 'donn√©es': 0, 'du': 1, 'd√©veloppement': 1, 'en': 0, 'est': 1, 'et': 2, 'favorise': 0, 'fondamental': 0, 'gestion': 0, 'informatique': 0, \"l'encapsulation\": 0, 'la': 2, 'largement': 1, 'le': 1, 'logiciels': 1, 'meilleure': 1, 'modularit√©': 0, 'modules': 1, 'm√©thodes': 0, 'objet': 1, 'objets': 0, 'organisation': 1, 'orient√©e': 1, 'ou': 0, 'paradigme': 1, 'permettant': 1, 'programmation': 2, 'projets': 0, 'qui': 0, 'repose': 1, 'r√©utilisation': 1, 'sur': 1, 'un': 1, 'une': 1, 'utilis√©': 1}\n"
     ]
    }
   ],
   "source": [
    "# afficher les dictionnaires\n",
    "dict1 = {word : frequency for word, frequency in sorted(dict1.items())}\n",
    "dict2 = {word : frequency for word, frequency in sorted(dict2.items())}\n",
    "print(\"dict1 =\",dict1)\n",
    "print(\"dict2 =\",dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110abfb",
   "metadata": {},
   "source": [
    "# produit cartesien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e68212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le produit cartesien D1.D2 = 27\n"
     ]
    }
   ],
   "source": [
    "def calculer_produit_cartesien(D1, D2):\n",
    "    \n",
    "    pc = sum(D1.get(mot, 0) * D2.get(mot, 0) for mot in D1)\n",
    "    return pc\n",
    "\n",
    "pc=calculer_produit_cartesien(dict1, dict2)\n",
    "print (\"le produit cartesien D1.D2 =\",pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915951f",
   "metadata": {},
   "source": [
    "# l'angle en degr√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7523e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Œ∏(ùê∑1,ùê∑2) = 58.3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def angle_Metric(D1, D2, pc):\n",
    "    # Calcul valeur absolue \"||d|| = racine(d*d)\" de D1 et D2\n",
    "    val_abs_D1 = math.sqrt(sum(frequence**2 for frequence in D1.values()))\n",
    "    val_abs_D2 = math.sqrt(sum(frequence**2 for frequence in D2.values()))\n",
    "    \n",
    "    # Calcul de la similarit√© cosinus\n",
    "    if val_abs_D1 == 0 or val_abs_D2 == 0:\n",
    "        return 0  # Pour √©viter la division par z√©ro\n",
    "    \n",
    "    similarite = pc / (val_abs_D1 * val_abs_D2)\n",
    "    \n",
    "    # V√©rifier si la similarit√© est en dehors de la plage (-1 √† 1)\n",
    "    similarite = min(max(similarite, -1), 1)\n",
    "    \n",
    "    # Calcul de l'angle entre les vecteurs en radians\n",
    "    angle_radians = math.acos(similarite)\n",
    "    \n",
    "    # Conversion de l'angle en degr√©s si n√©cessaire\n",
    "    angle_degres = math.degrees(angle_radians)\n",
    "    \n",
    "    return angle_degres\n",
    "\n",
    "angle_degres = angle_Metric(dict1, dict2, pc)\n",
    "angle_degres = round(angle_degres,2)\n",
    "\n",
    "print(\" Œ∏(ùê∑1,ùê∑2) =\", angle_degres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666c541",
   "metadata": {},
   "source": [
    "# Distance euclidienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a33bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_euclidienne 7.0\n"
     ]
    }
   ],
   "source": [
    "def distance_euclidienne(d1, d2):\n",
    "    somme_carres=0\n",
    "    for mot in d1.keys():\n",
    "        somme_carres += (d1[mot] - d2[mot]) ** 2\n",
    "    distance = math.sqrt(somme_carres) \n",
    "    return distance\n",
    "\n",
    "print('distance_euclidienne',distance_euclidienne(dict1, dict2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2ffde",
   "metadata": {},
   "source": [
    "# tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e464db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarit√© TF-IDF entre les dictionnaires : 27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calculer_similarity_tfidf(dict1, dict2):\n",
    "    # Cr√©er une liste de mots communs aux deux dictionnaires\n",
    "    mots_communs = set(dict1.keys()) & set(dict2.keys())\n",
    "\n",
    "    # Cr√©er des listes de poids TF-IDF correspondants pour les mots communs\n",
    "    poids_tfidf_dict1 = [dict1[mot] for mot in mots_communs]\n",
    "    poids_tfidf_dict2 = [dict2[mot] for mot in mots_communs]\n",
    "\n",
    "    # Calculer la somme des produits des poids TF-IDF correspondants\n",
    "    similarity = sum(w1 * w2 for w1, w2 in zip(poids_tfidf_dict1, poids_tfidf_dict2))\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "similarite_tfidf = calculer_similarity_tfidf(dict1, dict2)\n",
    "print(\"Similarit√© TF-IDF entre les dictionnaires :\", similarite_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a9c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarit√© TF-IDF (distance euclidienne) : 1.0627740613693037\n"
     ]
    }
   ],
   "source": [
    "#tf-idf distance euclidienne\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Cr√©ez les vecteurs TF-IDF pour les documents\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "# Transformez les matrices en vecteurs\n",
    "vector1 = tfidf_matrix[0].toarray()\n",
    "vector2 = tfidf_matrix[1].toarray()\n",
    "\n",
    "# Calculez la distance euclidienne\n",
    "euclidean_distance = np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "print(\"Similarit√© TF-IDF (distance euclidienne) :\", euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc349ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarit√© TF-IDF : 0.4352556472402977\n"
     ]
    }
   ],
   "source": [
    "#tf-idf cosinus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculer_similarity_tfidf(dict1, dict2):\n",
    "    # Convertir les dictionnaires en documents sous forme de texte\n",
    "    doc1 = ' '.join([mot for mot, frequence in dict1.items() for i in range(frequence)])\n",
    "    doc2 = ' '.join([mot for mot, frequence in dict2.items() for i in range(frequence)])\n",
    "\n",
    "    # Cr√©er un vecteuriseur TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transformer les documents en vecteurs TF-IDF\n",
    "    tfidf_matrix = vectorizer.fit_transform([doc1, doc2])\n",
    "    # Calculer la similarit√© cosinus entre les deux vecteurs TF-IDF\n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    return similarity[0][0]\n",
    "\n",
    "similarite_tfidf = calculer_similarity_tfidf(dict1, dict2)\n",
    "\n",
    "print(\"Similarit√© TF-IDF :\", similarite_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51e6b9",
   "metadata": {},
   "source": [
    "# Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d07b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance de Levenshtein  : 30\n"
     ]
    }
   ],
   "source": [
    "#matrice\n",
    "def levenshtein_distance(paragraph1, paragraph2):\n",
    "    # Divisez chaque paragraphe en mots (ou tokens)\n",
    "\n",
    "    len_words1 = len(tokendoc1)\n",
    "    len_words2 = len(tokendoc2)\n",
    "\n",
    "    # Cr√©ez une matrice pour stocker les distances interm√©diaires\n",
    "    dp = [[0] * (len_words2 + 1) for _ in range(len_words1 + 1)]\n",
    "\n",
    "    # Initialisez la premi√®re ligne et la premi√®re colonne\n",
    "    for i in range(len_words1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_words2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Remplissez la matrice en calculant les distances minimales entre les mots\n",
    "    for i in range(1, len_words1 + 1):\n",
    "        for j in range(1, len_words2 + 1):\n",
    "            if tokendoc1[i - 1] == tokendoc2[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    # La distance minimale est stock√©e dans dp[len_words1][len_words2]\n",
    "    return dp[len_words1][len_words2]\n",
    "\n",
    "distance = levenshtein_distance(doc1, doc2)\n",
    "print(f\"Distance de Levenshtein  : {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c2ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance de Levenshtein : 30\n"
     ]
    }
   ],
   "source": [
    "#tableau\n",
    "def levenshtein_distance(paragraph1, paragraph2):\n",
    "    len_words1 = len(tokendoc1)\n",
    "    len_words2 = len(tokendoc2)\n",
    "\n",
    "    # Cr√©ez une liste pour stocker les distances interm√©diaires\n",
    "    dp = [0] * (len_words2 + 1)\n",
    "\n",
    "    # Initialisez la premi√®re ligne\n",
    "    for j in range(len_words2 + 1):\n",
    "        dp[j] = j\n",
    "\n",
    "    # Remplissez la liste en calculant les distances minimales entre les mots\n",
    "    for i in range(1, len_words1 + 1):\n",
    "        prev = dp[0]  # Stockez la valeur pr√©c√©dente dp[i-1][0]\n",
    "        dp[0] = i\n",
    "        for j in range(1, len_words2 + 1):\n",
    "            if tokendoc1[i - 1] == tokendoc2[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            temp = dp[j]\n",
    "            dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + cost)\n",
    "            prev = temp\n",
    "\n",
    "    # La distance minimale est stock√©e dans dp[len_words2]\n",
    "    return dp[len_words2]\n",
    "\n",
    "distance = levenshtein_distance(doc1, doc2)\n",
    "print(f\"Distance de Levenshtein : {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb3a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
